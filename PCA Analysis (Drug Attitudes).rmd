---
title: "Principal Components Analysis on Drug Attitudes"
output: pdf_document
---

In this project, I apply PCA techniques on a dataset on drug attitudes in the United States to ascertain factors that account for most of the correlation across the data, i.e., which factors explain most of the drug attitude data.

```{r}
drug <- read.csv("Data/Drugattitudes.csv")
drug <- drug[complete.cases(drug), ]
```
Multivariate Normal Distribution
Although multivariate normality is not necessary to perform PCA, it can affect the interpretation of results, because the relative importance of skewed variables' components can be exaggerated or understated. In addition, it is a required assumption for parallel analysis.

A group of variables that are multivariate normally distributed consist of variables that are themselves normally distributed. Hence, we generate a boxplot of the individual variables, to ensure that their quartiles are consistent with that of a normal distribution, and that there is minimal skewdness.
```{r}
boxplot(drug, las = 2, main = "Boxplots of variables in drug attitudes data set")
```
We can see from the boxplot that it is difficult to assume multivariate normality, since there are some variables that are unlikely to have come from a normal distribution. However, it could be argued that this is to be expected, as there are only 5 possible values for each variable. The fact that these variables are discrete over a small range makes it difficult to interpret the boxplots.

Since it is difficult to deduce multivariate normality visually from the above graph, let's apply Mardia's multivariate normality test from the MVN package, and also generate a chi-squared quantile-quantile plot.
```{r}
library(MVN)
mardiaTest(drug, qqplot = TRUE)
```

We see that the above R output suggests that our data is in fact consistent with an assumption of multivariate normality.  Therefore, we continue to use the variables as they are when performing PCA, but also take some care in interpreting the loadings and when performing parallel analysis, due to the discrete nature of our variables.

Correlation Matrix

```{r}
drug_cor <- round(cor(drug),digits=2)
drug_cor
```

It seems that our data is a good candidate for PCA, as there are a number of variables that are highly correlated with each other. This means that we can reduce the dimensionality of the data set without loosing too much of its information. For example, the pairs of variables high and notuse, dope and notuse, and legal and trip all have correlation coefficients that are greater in magnitude than 0.65. Furthermore, the correlation matrix only tells us about the pairwise correlations - it might be the case that one of the variables can be written as a linear combination of two or more other variables.

On the other hand, there are variables that exhibit almost no correlation with each other. For example, less alcohol and side effects have a corrrelation coefficient of $<0.01$. Thus, this suggests that the data set actually contains useful information, and cannot be represented faithfully via only one or two principal components.

Principal Component Analysis

The following R code will carry out principal component analysis on the correlation matrix:
```{r}
pc1 <- princomp(drug, cor=TRUE)

#print(summary(pc1),digits=2,loadings=pc1$loadings,cutoff=0)
```
To decide how many principal components to retain, we first observe the proportion of variance explained:
```{r}
summary(pc1)
```
We observe that cumulatively, the first 8 principal components account for 80% of the variance.

Next we can view the eigenvalues of the principal components. Note that in R the value given is the standard deviation rather than the variance, so we square the values as seen below:
```{r}
(summary(pc1)$sdev)^2
```
It appears that the first 7 principal components have eigenvalues above 1.

Next, we check the screeplot to look for "elbows":
```{r}
screeplot(pc1,type="lines",col="red",lwd=2,pch=19,cex=1.2,main="Scree Plot")
```
We observe from the graph that two of the significant "elbows" in the plot are at the 2nd and the 4th principal component.

Thus, we will retain the first 3 principal components. This will explain about 52.4% of the variance, and all of the eigenvalues are greater than 1. Furthermore, these 3 principal components occur before the second "elbow" in the screeplot. The eigenvalues of the first 3 principal components are also above thresholds of the Longman and Allen methods.

Examine Loadings

The following code displays the loadings of the first three principal components:
```{r}
print(loadings(pc1,digits=2)[,1:3])
```
For the first principal component, the variables "notuse", "relationship", and "dope" had loadings with a magnitude of at least 0.3.

For the second principal component, the variables "legal", "dangerous", "psycho", "stoned", "calm", and "noaspiring" had loadings with magnitudes of above 0.3.

For the third principal component, the variables "legal", "trip", "experience", and "sideffects" had loadings with magnitudes of above 0.3.
